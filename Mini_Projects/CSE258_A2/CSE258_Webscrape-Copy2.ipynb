{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scrape valid surf rides from the Smartfin website: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#Imports \n",
    "import requests\n",
    "import lxml.html as lh\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape verified surf sessions into a dataframe (can add more later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by just looking at my surf sessions in SD (near CDIP buoy): \n",
    "\n",
    "# 15218 - First VIRB filmed session - Oct. 24, 2018\n",
    "# 15669 - Second VIRB filmed session - Nov. 7, 2018\n",
    "# 15692 - Third VIRB filmed session - Nov. 9, 2018\n",
    "# 15686 - Fourth VIRB filmed session - Nov. 11, 2018\n",
    "\n",
    "ride_ids = [15218, 15669, 15692, 15686]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get urls for surf sessions in Scripps beach area that most likely correspond to surfing time intervals: \n",
    "url_45_60 = 'https://surf.smartfin.org/advanced_search/?northEastLat=32.6&northEastLon=-117.24&southWestLat=32.9&southWestLon=-117.33&dateTimeBegin=&dateTimeEnd=&timeZone=PDT&durationMin=45&durationMax=60&sensorTypeWave=true#searchResultsContainer'\n",
    "url_61_80 = 'https://surf.smartfin.org/advanced_search/?northEastLat=32.6&northEastLon=-117.24&southWestLat=32.9&southWestLon=-117.33&dateTimeBegin=&dateTimeEnd=&timeZone=PDT&durationMin=61&durationMax=80&sensorTypeWave=true#searchResultsContainer'\n",
    "url_81_100 = 'https://surf.smartfin.org/advanced_search/?northEastLat=32.6&northEastLon=-117.24&southWestLat=32.9&southWestLon=-117.33&dateTimeBegin=&dateTimeEnd=&timeZone=PDT&durationMin=81&durationMax=100&sensorTypeWave=true#searchResultsContainer'\n",
    "url_101_130 = 'https://surf.smartfin.org/advanced_search/?northEastLat=32.6&northEastLon=-117.24&southWestLat=32.9&southWestLon=-117.33&dateTimeBegin=&dateTimeEnd=&timeZone=PDT&durationMin=101&durationMax=130&sensorTypeWave=true#searchResultsContainer'\n",
    "\n",
    "urls = [url_61_80]\n",
    "#urls = [url_45_60, url_61_80, url_81_100, url_101_130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will parse rows from data tables on CDIP's website:\n",
    "def parse_rows(row_number):\n",
    "    name_string = \"\"\n",
    "    name_list = []\n",
    "    for t in tr_elements[row_number]:\n",
    "        name = t.text_content()\n",
    "        for i in name: \n",
    "            name_string += i\n",
    "    #print(name_string)\n",
    "\n",
    "    # Create a string from the values\n",
    "    name_string = name_string.split(\" \")\n",
    "    \n",
    "    # Remove all spaces from the list\n",
    "    for i in name_string: \n",
    "        if len(i) > 0:\n",
    "            name_list.append(i)\n",
    "      \n",
    "    # Ensure that time stays with 'Date (UTC)'' header\n",
    "    name_list[0] = name_list[0] + \" \" +  name_list[1]\n",
    "    name_list.pop(1)\n",
    "    \n",
    "    return name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n",
      "[' 16302', ' 16301', ' 16298', ' 16290', ' 16262', ' 16243', ' 16237', ' 16236', ' 16196', ' 16195', ' 16192', ' 16168', ' 16161', ' 16148', ' 16073', ' 16072', ' 16000', ' 15972', ' 15947', ' 15859']\n",
      "[' 10191', ' 10157', ' 10085', ' 10084', ' 10068', ' 10060', ' 10040', ' 10036', ' 10015', ' 10008']\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each url and get all ride_ids located at that url: \n",
    "ride_ids = []\n",
    "for u in urls: \n",
    "    \n",
    "    # Create a handle, page, to handle the contents of the website\n",
    "    page = requests.get(u)\n",
    "\n",
    "    # Store the contents of the website under doc\n",
    "    doc = lh.fromstring(page.content)\n",
    "\n",
    "    # Parse data stored between <tr>..</tr> of HTML\n",
    "    tr_elements = doc.xpath('//tr')\n",
    "\n",
    "    # Retrieve all of the ride ids: \n",
    "    for j in range(1, len(tr_elements)):\n",
    "        data = parse_rows(j)\n",
    "        data = str(data[0]).strip(\"\\n\")\n",
    "        if int(data) > 10000: \n",
    "            ride_ids.append(data)\n",
    "\n",
    "print(len(ride_ids))\n",
    "print(ride_ids[:20])\n",
    "print(ride_ids[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fin ID Scraper: \n",
    "\n",
    "ride_url_base = 'http://surf.smartfin.org/ride/'\n",
    "str_id_csv = 'img id=\"temperatureChart\" class=\"chart\" src=\"' # Look for this text in the HTML contents in fcn below\n",
    "\n",
    "def read_csv_from_ride2(ride): \n",
    "    smartfin_url_base = 'http://surf.smartfin.org'\n",
    "    ride_url = ride_url_base + str(ride)\n",
    "    html_page = requests.get(ride_url)\n",
    "    soup = BeautifulSoup(html_page.text, 'html.parser')\n",
    "    #print(soup)\n",
    "    links = []\n",
    "    for link in soup.findAll('a'):\n",
    "        links.append(link)\n",
    "    if len(links) > 8: \n",
    "        motion_csv_link = str(links[8])\n",
    "        #print(links[8])\n",
    "        motion_csv_link = motion_csv_link.split(\" \")\n",
    "        motion_csv_link = motion_csv_link[1]\n",
    "        motion_csv_link = motion_csv_link.split(\"\\\"\")\n",
    "        motion_csv_link = motion_csv_link[1]\n",
    "\n",
    "        #print(motion_csv_link)\n",
    "\n",
    "        motion_csv_link = smartfin_url_base + motion_csv_link\n",
    "        motion_df_small = pd.read_csv(motion_csv_link, parse_dates = [0])\n",
    "\n",
    "        return 1, motion_df_small\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "1 20621\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "Total number of rides downloaded:  1\n"
     ]
    }
   ],
   "source": [
    "appended_motion_list = []\n",
    "appended_multiIndex = [] # fin_id & ride_id used to identify each DataFrame\n",
    "\n",
    "counter = 0\n",
    "index = 1\n",
    "for ride in ride_ids[10:30]: \n",
    "    #ride = '16167'\n",
    "    b, motion_df = read_csv_from_ride2(ride)\n",
    "    # Check to make sure that a .CSV link was found for motion: \n",
    "    if b == 1: \n",
    "        counter += 1\n",
    "        print(counter, len(motion_df))\n",
    "        motion_df.set_index('UTC', drop = True, append = False, inplace = True)\n",
    "        appended_multiIndex.append(str(ride)) # build list to be multiIndex of future DataFrame\n",
    "        appended_motion_list.append(motion_df)\n",
    "    print(index)\n",
    "    index += 1\n",
    "\n",
    "print(\"Total number of rides downloaded: \", counter)\n",
    "df_keys = tuple(appended_multiIndex)\n",
    "motion_df = pd.concat(appended_motion_list, keys = df_keys, names = ['ride_id'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "Total number of rides downloaded:  0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0af95f1a61c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total number of rides downloaded: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdf_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappended_multiIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmotion_df2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappended_motion_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ride_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    226\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                        copy=copy, sort=sort)\n\u001b[0m\u001b[1;32m    229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No objects to concatenate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "appended_motion_list = []\n",
    "appended_multiIndex = [] # fin_id & ride_id used to identify each DataFrame\n",
    "\n",
    "counter = 0\n",
    "index = 1\n",
    "for ride in ride_ids[30:60]: \n",
    "    #ride = '16167'\n",
    "    b, motion_df = read_csv_from_ride2(ride)\n",
    "    # Check to make sure that a .CSV link was found for motion: \n",
    "    if b == 1: \n",
    "        counter += 1\n",
    "        print(counter, len(motion_df))\n",
    "        motion_df.set_index('UTC', drop = True, append = False, inplace = True)\n",
    "        appended_multiIndex.append(str(ride)) # build list to be multiIndex of future DataFrame\n",
    "        appended_motion_list.append(motion_df)\n",
    "    print(index)\n",
    "    index += 1\n",
    "\n",
    "print(\"Total number of rides downloaded: \", counter)\n",
    "df_keys = tuple(appended_multiIndex)\n",
    "motion_df2 = pd.concat(appended_motion_list, keys = df_keys, names = ['ride_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_df2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "appended_ocean_list = [] # list of DataFrames from original CSVs\n",
    "appended_motion_list = []\n",
    "appended_multiIndex = [] # fin_id & ride_id used to identify each DataFrame\n",
    "\n",
    "print(\"Once the counter gets to \", len(ride_ids), \" it will be done printing.\")\n",
    "\n",
    "## Nested loops (for each fin ID, find all ride IDs, then build a DataFrame from all ride CSVs)\n",
    "## (Here, ride IDS are either ocean or motion dataframes)\n",
    "count_good_fins = 0\n",
    "    \n",
    "# Loop over ride_ids and find CSVs\n",
    "counter = 0\n",
    "for rid in ride_ids:\n",
    "    counter += 1\n",
    "    if counter % 10 == 0:\n",
    "        print(counter)\n",
    "    try:\n",
    "        new_motion_df = read_csv_from_ride(rid) # get given ride's CSV from its ride ID using function above\n",
    "        new_motion_df.set_index('UTC', drop = True, append = False, inplace = True)            \n",
    "\n",
    "        if not new_motion_df.empty: # Calibration rides, for example\n",
    "            # Append only if DF isn't empty. There may be a better way to control empty DFs which are created above\n",
    "            appended_multiIndex.append(str(rid)) # build list to be multiIndex of future DataFrame\n",
    "            #appended_ocean_list.append(new_ocean_df)\n",
    "            appended_motion_list.append(new_motion_df)\n",
    "            #print(\"Ride data has been uploaded.\")\n",
    "            #print(\"Ride: \", rid, \"data has been uploaded.\")\n",
    "            count_good_fins += 1\n",
    "            #print(\"Ride \", rid, \"worked!\")    \n",
    "     \n",
    "    except: \n",
    "        print(\"Ride \", rid, \"threw an exception!\")    \n",
    "\n",
    "#%% Build the \"Master\" DataFrame\n",
    "\n",
    "# appended_ocean_df.summary()\n",
    "\n",
    "df_keys = tuple(appended_multiIndex) # keys gotta be a tuple, a list which data in it cannot be changed\n",
    "#ocean_df = pd.concat(appended_ocean_list, keys = df_keys, names=['ride_id'])\n",
    "motion_df = pd.concat(appended_motion_list, keys = df_keys, names = ['ride_id'])\n",
    "\n",
    "\n",
    "##Here, maybe just use info from the motion_df and don't worry about ocean_df data for now.\n",
    "##If you do want ocean_df data, look at how Phil was getting it from \"July 10th and 11th Calibration\" jupyter notebook file.\n",
    "#print(motion_df)\n",
    "#print(appended_motion_list)\n",
    "print(\"Done.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View data in a python dataframe: \n",
    "print(len(motion_df))\n",
    "motion_df[0:10]\n",
    "\n",
    "# I think that we're resampling each surf session at too small of a rate? \n",
    "# Why do we have so little data? \n",
    "\n",
    "#Sampling interval of 33ms: 3,290,682 initial data points, 438,099 final data points (adding .033 to UTC time)\n",
    "#Sampling interval of 20ms: 5,429,605 initial data points, 438,099 final data points (adding .020 to UTC time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop latitude/longitude columns and remove NaN rows: \n",
    "motion_df = motion_df.drop([\"Latitude\", \"Longitude\"], axis=1)\n",
    "motion_df = motion_df.dropna(axis=0, how='any')\n",
    "print(len(motion_df))\n",
    "motion_df[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape data from same-day CDIP and add it as an additional column to the dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format should be appending year, month\n",
    "# Example: Sept. 2019 -> 201909\n",
    "def create_url_string(year, month):\n",
    "    url='https://cdip.ucsd.edu/themes/cdip?tz=UTC&numcolorbands=10&palette=cdip_classic&zoom=auto&ll_fmt=dm&high=6.096&r=999&un=1&pb=1&d2=p70&u2=s:201:st:1:v:parameter:dt:'\n",
    "    if 2014 <= int(year) <= 2019:\n",
    "        year = year\n",
    "    if 1 <= int(month) <= 12 and len(month) == 2:\n",
    "        month = month\n",
    "    url += year\n",
    "    url += month\n",
    "    return url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each row in the dataframe, parse the index ('UTC' column) to get the year, month information. \n",
    "motion_df.columns\n",
    "\n",
    "dates = []\n",
    "times = []\n",
    "for row in motion_df.index:\n",
    "    time = str(row[1])\n",
    "    date = time.split(\" \")\n",
    "    time = date[1]\n",
    "    date = date[0]\n",
    "    time = time.split(\".\")\n",
    "    time = time[0]\n",
    "    time = time.split(\":\")\n",
    "    time = time[0] + \":\" + time[1]\n",
    "    times.append(time)\n",
    "    dates.append(date)\n",
    "print(dates[0:10], times[0:10])\n",
    "\n",
    "motion_df[\"Date\"] = dates\n",
    "motion_df[\"Time\"] = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will find the closest time on the CDIP table to the surf ride's time: \n",
    "def find_closest_single_time(date, time_utc):\n",
    "    time_list = []\n",
    "    \n",
    "    # Iterate over each date\n",
    "    for x in df_data['Date(UTC)']:\n",
    "        a = x.split(\" \")\n",
    "        if date == a[0]:\n",
    "            a = str(a[1]).split(':')\n",
    "            y = str(time_utc).split(\":\")\n",
    "            #print(a)\n",
    "            #print(y)\n",
    "            \n",
    "            # Find the closest timestamp (less than 15 minutes away)\n",
    "            time1 = int(a[0])*60 + int(a[1])\n",
    "            time2 = int(y[0])*60 + int(y[1])\n",
    "            \n",
    "            if abs(time1 - time2) <= 15:\n",
    "                time_string = str(a[0]) + \":\" + str(a[1])\n",
    "                time_list.append(time_string)\n",
    "           \n",
    "    return time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the output labels from CDIP data for each Smartfin timestamp: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output labels from CDIP data for each Smartfin timestamp: \n",
    "old_url = 'old_url'\n",
    "old_date = 'old_date'\n",
    "old_time = 'xx:xx'\n",
    "\n",
    "wave_heights = []\n",
    "wave_periods = []\n",
    "wave_directions = []\n",
    "for d, time in zip(dates,times):\n",
    "    di = d.split(\"-\")\n",
    "    year = di[0]\n",
    "    month = di[1]\n",
    "    url = create_url_string(year, month)\n",
    "    \n",
    "    #We need to load a new table since we have a new surf ride session:\n",
    "    if (url != old_url): \n",
    "        old_url = url\n",
    "        # Create a handle, page, to handle the contents of the website\n",
    "        page = requests.get(url)\n",
    "\n",
    "        # Store the contents of the website under doc\n",
    "        doc = lh.fromstring(page.content)\n",
    "\n",
    "        # Parse data stored between <tr>..</tr> of HTML\n",
    "        tr_elements = doc.xpath('//tr')\n",
    "\n",
    "        # Need to drop the first 3 rows since they aren't in the table\n",
    "        tr_elements = tr_elements[3:]\n",
    "        \n",
    "        # Parse the first row as the header\n",
    "        tr_elements = doc.xpath('//tr')\n",
    "\n",
    "        # Create empty list\n",
    "        headers = []\n",
    "        i = 0\n",
    "\n",
    "        # For each row, store each first element (header) and an empty list\n",
    "        for t in tr_elements[3]:\n",
    "            i+=1\n",
    "            name=t.text_content()\n",
    "            #print(name)\n",
    "            headers.append(name)\n",
    "\n",
    "        #print(headers)\n",
    "\n",
    "        # Create a Pandas dataframe: \n",
    "        data_list = []\n",
    "\n",
    "        #Since out first row is the header, data is stored on the second row onwards\n",
    "        for j in range(4, len(tr_elements)):\n",
    "            data = parse_rows(j)\n",
    "            data_list.append(data)\n",
    "\n",
    "        df_data = pd.DataFrame(data_list, columns=headers)\n",
    "        #print(df_data[:10])\n",
    "        \n",
    "        #Find the closest time in the CDIP data table that corresponds to the Smartfin data: \n",
    "        date = d #YYYY-MM-DD format\n",
    "        print(\"date, time \", date, time)\n",
    "        time_list = find_closest_single_time(date, time)\n",
    "        print(\"time list \", time_list)\n",
    "        \n",
    "        # Now compute the average significant wave height for that time period:\n",
    "        length = df_data['Date(UTC)'].size\n",
    "        wave_height_list = []\n",
    "        wave_period_list = []\n",
    "        wave_direction_list = []\n",
    "        for time in time_list: \n",
    "            date_data = date + \" \" + time\n",
    "            for i in range(0, length):\n",
    "                if df_data['Date(UTC)'][i] == date_data:\n",
    "                    print(\"height \", df_data['Hs(ft)'][i])\n",
    "                    wave_height_list.append(float(df_data['Hs(ft)'][i]))\n",
    "                    wave_period_list.append(float(df_data['Tp(s)'][i]))\n",
    "                    wave_direction_list.append(float(df_data['Dp(deg)'][i]))\n",
    "                    \n",
    "        # Throws error when date doesn't exist on CDIP data (ex: 2018-01-18)\n",
    "        if len(wave_height_list) == 0 or len(wave_period_list) == 0 or len(wave_direction_list) == 0: \n",
    "            avg_wave_height = np.nan\n",
    "            avg_wave_period = np.nan\n",
    "            avg_wave_direction = np.nan\n",
    "            \n",
    "        else: \n",
    "            #print(wave_height_list)\n",
    "            avg_wave_height = sum(wave_height_list)/len(wave_height_list)\n",
    "            avg_wave_period = sum(wave_period_list)/len(wave_period_list)\n",
    "            avg_wave_direction = sum(wave_direction_list)/len(wave_direction_list)\n",
    "            #print(avg_wave_height)\n",
    "            #print(avg_wave_period)\n",
    "            #print(avg_wave_direction) \n",
    "            \n",
    "    # Same date/month but maybe a different day or time:\n",
    "    elif old_date != date or old_time[3] != time[3]:\n",
    "        date = d #YYYY-MM-DD format\n",
    "        #print(date, time)\n",
    "        time_list = find_closest_single_time(date, time)\n",
    "        #print(time_list)\n",
    "        \n",
    "        old_date = date\n",
    "        old_time = time\n",
    "        \n",
    "        # Now compute the average significant wave height for that time period:\n",
    "        length = df_data['Date(UTC)'].size\n",
    "        wave_height_list = []\n",
    "        wave_period_list = []\n",
    "        wave_direction_list = []\n",
    "        for time in time_list: \n",
    "            date_data = date + \" \" + time\n",
    "            for i in range(0, length):\n",
    "                if df_data['Date(UTC)'][i] == date_data:\n",
    "                    #print(df_data['Hs(ft)'][i])\n",
    "                    wave_height_list.append(float(df_data['Hs(ft)'][i]))\n",
    "                    wave_period_list.append(float(df_data['Tp(s)'][i]))\n",
    "                    wave_direction_list.append(float(df_data['Dp(deg)'][i]))\n",
    "                    \n",
    "      \n",
    "        # Throws error when date doesn't exist on CDIP data (ex: 2018-01-18)\n",
    "        if len(wave_height_list) == 0 or len(wave_period_list) == 0 or len(wave_direction_list) == 0: \n",
    "            avg_wave_height = np.nan\n",
    "            avg_wave_period = np.nan\n",
    "            avg_wave_direction = np.nan\n",
    "            \n",
    "        else: \n",
    "            #print(wave_height_list)\n",
    "            avg_wave_height = sum(wave_height_list)/len(wave_height_list)\n",
    "            avg_wave_period = sum(wave_period_list)/len(wave_period_list)\n",
    "            avg_wave_direction = sum(wave_direction_list)/len(wave_direction_list)\n",
    "            #print(avg_wave_height)\n",
    "            #print(avg_wave_period)\n",
    "            #print(avg_wave_direction)    \n",
    "\n",
    "    # If they have the same url, date, and time then they will have the same value computed. \n",
    "    wave_heights.append(avg_wave_height)\n",
    "    wave_periods.append(avg_wave_period)\n",
    "    wave_directions.append(avg_wave_direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dates))\n",
    "print(len(wave_heights))\n",
    "print(len(wave_periods))\n",
    "print(len(wave_directions))\n",
    "\n",
    "\n",
    "# Check to make sure different wave heights were appended throughout the dataframe: \n",
    "motion_df[\"Hs(ft)\"] = wave_heights\n",
    "motion_df[\"Tp(s)\"] = wave_periods\n",
    "motion_df[\"Dp(deg)\"] = wave_directions\n",
    "\n",
    "for x in range(0, 60000, 10000):\n",
    "    print(motion_df.iloc[[x]]['Hs(ft)'])\n",
    "    print(motion_df.iloc[[x]]['Tp(s)'])\n",
    "    print(motion_df.iloc[[x]]['Dp(deg)'])\n",
    "\n",
    "motion_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now drop all of the nan values that I introduced earlier: \n",
    "motion_df = motion_df.dropna(axis=0, how='any')\n",
    "print(len(motion_df))\n",
    "motion_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're collecting IMU data at 3-4 Hz here. \n",
    "# We're using 4 surf sessions and we have 60,000 data points. \n",
    "# If we only calculated Hs once per minute then we would have 330 data points. \n",
    "\n",
    "# Instead of looking at significant wave height, we could look at wave direction \n",
    "# or we could look at Hs, dir, and period and see which one we predict best. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw IMU values to real values: \n",
    "g_const = 512 #Raw acceleration constant 512 = 1g (accelerometer's measured force due to gravity)\n",
    "gravity = 9.80665 #Approximate measurement for gravity\n",
    "\n",
    "# Correct the IMU Acceleration columns into units of meters\n",
    "# Dividing by 512 is equivalent to muliplying by 4 to correct the bit shifting by 2 places and dividing by 2048 to convert bits to G's\n",
    "# Multiplying by the 9.81 afterwards is simply to convert G's into m/s^2\n",
    "motion_df['IMU A1'] = motion_df['IMU A1'].apply(lambda x: x / g_const * gravity)\n",
    "motion_df['IMU A2'] = motion_df['IMU A2'].apply(lambda x: x / g_const * gravity)\n",
    "motion_df['IMU A3'] = motion_df['IMU A3'].apply(lambda x: x / g_const * gravity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gyroscopic Rotation converts to deg/s\n",
    "gyro_const = 8.2 # Raw gyrscope constant 8.2 bits per degree\n",
    "motion_df['IMU G1'] = motion_df['IMU G1'].apply(lambda x: x / gyro_const)\n",
    "motion_df['IMU G2'] = motion_df['IMU G2'].apply(lambda x: x / gyro_const)\n",
    "motion_df['IMU G3'] = motion_df['IMU G3'].apply(lambda x: x / gyro_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnetometer values\n",
    "# Offset variables help in recentering the magnetic data in order to define direction and use trig functions\n",
    "'''\n",
    "M1_offset_var = 219.786\n",
    "M2_offset_var = 180\n",
    "M3_offset_var = 280\n",
    "\n",
    "motion_df['IMU M1'] = motion_df['IMU M1'].apply(lambda x: x - M1_offset_var)\n",
    "motion_df['IMU M2'] = motion_df['IMU M2'].apply(lambda x: x - M2_offset_var)\n",
    "motion_df['IMU M3'] = motion_df['IMU M3'].apply(lambda x: x - M3_offset_var)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(motion_df))\n",
    "motion_df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After converting all to real values, try to export data to .CSV so everyone else doesn't have to webscrape it:\n",
    "#motion_df.to_csv('CSE258_A2_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#motion_df.to_csv('CSE258_A2_Data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
